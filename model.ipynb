{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma=WordNetLemmatizer()\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,SimpleRNN,Flatten,Embedding,Masking,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import relu,sigmoid,softmax\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Ngb5bqcqHiWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPExGzlOHnEs",
        "outputId": "304211a9-ccfb-497d-b254-dd14f84c473b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-7LCtpsG5U5",
        "outputId": "24c4c974-d9e8-469e-e1eb-dd22a98c21e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape(10000, 2)\n",
            "Index(['review', 'sentiment'], dtype='object')\n",
            "review       0\n",
            "sentiment    0\n",
            "dtype: int64\n",
            "                                              review  sentiment\n",
            "0  One of the other reviewers has mentioned that ...          1\n",
            "1  A wonderful little production. <br /><br />The...          1\n",
            "2  I thought this was a wonderful way to spend ti...          1\n",
            "3  Basically there's a family where a little boy ...          0\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
            "sentiment\n",
            "1    5028\n",
            "0    4972\n",
            "Name: count, dtype: int64\n",
            "data after cleaning:['one reviewer mentioned watching 1 oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home manyaryans muslim gangsta latino christian italian irish moreso scuffle death stare dodgy dealing shady agreement never far awaybr br would say main appeal show due fact go show wouldnt dare forget pretty picture painted mainstream audience forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard wholl sold nickel inmate wholl kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side', 'wonderful little production br br filming technique unassuming oldtimebbc fashion give comforting sometimes discomforting sense realism entire piece br br actor extremely well chosen michael sheen got polari voice pat truly see seamless editing guided reference williams diary entry well worth watching terrificly written performed piece masterful production one great master comedy life br br realism really come home little thing fantasy guard rather use traditional dream technique remains solid disappears play knowledge sens particularly scene concerning orton halliwell set particularly flat halliwells mural decorating every surface terribly well done', 'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching lighthearted comedy plot simplistic dialogue witty character likable even well bread suspected serial killer may disappointed realize match point 2 risk addiction thought proof woody allen still fully control style many u grown lovebr br id laughed one woodys comedy year dare say decade ive never impressed scarlet johanson managed tone sexy image jumped right average spirited young womanbr br may crown jewel career wittier devil wear prada interesting superman great comedy go see friend', 'basically there family little boy jake think there zombie closet parent fighting timebr br movie slower soap opera suddenly jake decides become rambo kill zombiebr br ok first youre going make film must decide thriller drama drama movie watchable parent divorcing arguing like real life jake closet totally ruin film expected see boogeyman similar movie instead watched drama meaningless thriller spotsbr br 3 10 well playing parent descent dialog shot jake ignore', 'petter matteis love time money visually stunning film watch mr mattei offer u vivid portrait human relation movie seems telling u money power success people different situation encounter br br variation arthur schnitzlers play theme director transfer action present time new york different character meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitatbr br thing one get soul picture different stage loneliness one inhabits big city exactly best place human relation find sincere fulfillment one discerns case people encounterbr br acting good mr matteis direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make character come alivebr br wish mr mattei good luck await anxiously next work', 'probably alltime favorite movie story selflessness sacrifice dedication noble cause preachy boring never get old despite seen 15 time last 25 year paul lukas performance brings tear eye bette davis one truly sympathetic role delight kid grandma say like dressedup midget child make fun watch mother slow awakening whats happening world roof believable startling dozen thumb theyd movie', 'sure would like see resurrection dated seahunt series tech today would bring back kid excitement mei grew black white tv seahunt gunsmoke hero every weekyou vote comeback new sea huntwe need change pace tv would work world water adventureoh way thank outlet like view many viewpoint tv many moviesso ole way believe ive got wanna saywould nice read plus point sea huntif rhyme would 10 line would let submitor leave doubt quitif must go let', 'show amazing fresh innovative idea 70 first aired first 7 8 year brilliant thing dropped 1990 show really funny anymore continued decline complete waste time todaybr br truly disgraceful far show fallen writing painfully bad performance almost bad mildly entertaining respite guesthosts show probably wouldnt still air find hard believe creator handselected original cast also chose band hack followed one recognize brilliance see fit replace mediocrity felt must give 2 star respect original cast made show huge success show awful cant believe still air', 'encouraged positive comment film looking forward watching film bad mistake ive seen 950 film truly one worst awful almost every way editing pacing storyline acting soundtrack film song lame country tune played less four time film look cheap nasty boring extreme rarely happy see end credit film br br thing prevents giving 1score harvey keitel far best performance least seems making bit effort one keitel obsessive', 'like original gut wrenching laughter like movie young old love movie hell even mom liked itbr br great camp']\n",
            "data after one hot[[2872, 9243, 1784, 5926, 9035, 6164, 9412, 2813, 5562, 7631, 4821, 7945, 1446, 7170, 3106, 737, 9450, 6164, 230, 7714, 3584, 983, 8774, 7631, 1874, 9842, 1995, 8512, 5482, 2507, 2788, 8512, 9385, 5924, 1190, 5584, 4344, 983, 9473, 7657, 7597, 6111, 7170, 4212, 6164, 7988, 2558, 4951, 5258, 9429, 3829, 4736, 359, 7542, 7501, 4170, 2993, 9734, 2758, 8724, 4471, 7840, 1085, 881, 5856, 3561, 5505, 4855, 4170, 7872, 1975, 2470, 9232, 8672, 9927, 3280, 3843, 8905, 3587, 1273, 6296, 3914, 3231, 7607, 5376, 7166, 8100, 5018, 7170, 8185, 9828, 6385, 843, 8512, 9825, 7255, 9842, 8512, 3488, 5315, 1581, 7582, 3572, 7564, 5435, 3516, 1581, 8640, 1581, 1160, 7438, 9128, 6211, 3106, 9412, 7986, 5887, 9450, 4354, 7185, 4156, 9828, 1717, 5133, 9890, 8411, 6164, 772, 261, 3561, 3645, 5123, 983, 983, 3736, 8343, 4357, 3503, 2699, 5836, 4803, 3503, 3410, 3188, 8045, 4022, 8127, 3445, 2272, 9763, 4803, 9715, 2758, 2829, 9825, 9656, 9144, 3240, 2758, 4217, 5926, 6164, 5126, 4967, 6104, 1551, 1452, 8045, 6247, 7916, 3767], [5285, 7047, 76, 7170, 7170, 1465, 3409, 5004, 3021, 5147, 232, 8923, 8420, 1285, 5568, 3918, 9463, 4153, 7170, 7170, 6695, 6451, 8127, 5682, 9204, 8041, 772, 1782, 7406, 38, 5442, 9969, 5277, 5689, 1530, 891, 2096, 8229, 3818, 8127, 9851, 5926, 1736, 8585, 9796, 4153, 181, 76, 2872, 1476, 4816, 3242, 2895, 7170, 7170, 3918, 8081, 2978, 7872, 7047, 737, 4898, 4357, 148, 7597, 7663, 2146, 3409, 3554, 7977, 439, 5274, 9828, 2496, 7713, 3584, 5289, 8400, 3100, 8774, 7713, 2963, 8676, 5957, 7389, 2153, 2009, 2798, 8127, 9448], [4067, 5285, 5180, 2901, 2994, 5492, 7371, 6010, 2284, 779, 5753, 5111, 5926, 5023, 3242, 7499, 4082, 7056, 7234, 6677, 3620, 5651, 8127, 5272, 461, 1012, 41, 5126, 5344, 5551, 9031, 4477, 9725, 4007, 204, 4067, 4062, 2199, 6253, 4446, 3430, 9429, 8808, 915, 8691, 7020, 4679, 7170, 7863, 7953, 2872, 524, 3242, 5475, 5315, 9828, 4613, 2250, 7166, 3041, 9865, 4280, 7831, 6727, 317, 3610, 9005, 7631, 5615, 3760, 3841, 4026, 7170, 5126, 7992, 7531, 3359, 4915, 5390, 5478, 2159, 8147, 9010, 1476, 3242, 9842, 9969, 6035], [7591, 8449, 8902, 7047, 4799, 9827, 7913, 8449, 9790, 375, 447, 1215, 1319, 7170, 6101, 3311, 3825, 1416, 7798, 9827, 3198, 4967, 6337, 3410, 7129, 7170, 9875, 3106, 2962, 150, 9759, 8092, 6847, 496, 6759, 9363, 9363, 6101, 4284, 447, 8493, 3174, 6263, 1230, 2895, 9827, 375, 6127, 4974, 8092, 3083, 9969, 6366, 3028, 6101, 84, 5133, 9363, 4514, 6759, 1600, 7170, 4430, 5395, 8127, 2932, 447, 4247, 4097, 5747, 9827, 88], [4034, 7519, 7165, 2994, 7892, 9841, 821, 8092, 3029, 9390, 2419, 5307, 8691, 7876, 838, 3098, 7376, 6101, 9647, 6392, 8691, 7892, 3380, 2314, 1222, 3059, 1366, 7666, 7170, 7170, 8399, 4258, 3789, 5274, 1942, 2034, 4647, 8354, 4605, 2994, 5923, 4494, 3059, 6677, 505, 8058, 2872, 6564, 2872, 5180, 6019, 8974, 3251, 2872, 9647, 9368, 8426, 4477, 7770, 869, 8092, 1380, 1615, 1648, 763, 9969, 1222, 6887, 411, 6887, 9921, 7170, 737, 2872, 8045, 2171, 3572, 3059, 9618, 8051, 2872, 2369, 5000, 4170, 4821, 8052, 1437, 3098, 7376, 5969, 8706, 9518, 2872, 317, 3433, 1222, 7615, 7170, 9356, 5029, 9390, 7519, 5027, 1226, 2893, 5678, 1666, 7782, 8368, 9204, 3293, 624, 6983, 9713, 4957, 5011, 9759, 6677, 2978, 9689, 7170, 9125, 9390, 2419, 5029, 5063, 1028, 3171, 8974, 4544], [7136, 709, 7917, 6101, 9205, 2776, 8323, 5515, 359, 9015, 6672, 3590, 7166, 8045, 183, 4054, 1579, 4514, 2994, 4404, 7150, 5475, 4400, 2012, 9052, 4734, 534, 2089, 3567, 246, 2872, 5442, 2314, 2803, 8450, 6312, 4595, 9828, 6263, 2577, 4708, 7832, 9759, 7207, 3029, 6512, 918, 5525, 4043, 6108, 411, 8595, 1509, 3215, 1441, 8586, 3973, 6101], [6857, 8185, 6263, 9969, 7196, 7831, 5369, 5894, 6366, 7962, 8185, 5828, 1784, 6312, 4323, 7189, 1574, 4772, 6997, 9692, 5369, 122, 8712, 2153, 9018, 5295, 3399, 5923, 4510, 5838, 6237, 9198, 2854, 9692, 8185, 4544, 411, 1481, 420, 5180, 7217, 4806, 6263, 8554, 915, 7776, 9692, 915, 6883, 9428, 5180, 4794, 2250, 772, 7916, 4666, 6904, 4907, 5344, 4477, 4510, 6159, 9209, 8185, 5395, 6436, 8185, 2279, 2431, 3423, 9918, 329, 6847, 9842, 2279], [8512, 609, 4218, 3683, 458, 9668, 3106, 6509, 3106, 7096, 2728, 5475, 8465, 737, 2528, 7104, 8512, 8081, 90, 5817, 2765, 6507, 462, 541, 2994, 2740, 7170, 5442, 1596, 8100, 8512, 2731, 1216, 4150, 4510, 9052, 5079, 4510, 9974, 7016, 24, 6271, 8512, 7136, 3488, 4446, 779, 5969, 4756, 4794, 3522, 3490, 4223, 5011, 9023, 7974, 4911, 9463, 9403, 2872, 798, 97, 9969, 5777, 2712, 50, 6294, 6847, 232, 9725, 2843, 7412, 4223, 5011, 2422, 8512, 2184, 2314, 8512, 7813, 7618, 4794, 4446, 779], [5894, 1158, 1614, 8092, 1558, 3677, 5926, 8092, 4510, 289, 2250, 1579, 8286, 8092, 5442, 2872, 4794, 7813, 5079, 2153, 5180, 5689, 2122, 3757, 9356, 626, 8092, 7238, 241, 9563, 7082, 7541, 6208, 7552, 2994, 8092, 1648, 5063, 4354, 3590, 6914, 129, 4586, 9969, 5502, 6530, 8092, 7170, 7170, 737, 6870, 8647, 4988, 9513, 8756, 8100, 8052, 9052, 9779, 9647, 2069, 6193, 216, 2872, 8756, 9833], [6263, 4223, 4519, 347, 9197, 6263, 6101, 3841, 183, 7165, 6101, 7026, 5651, 877, 32, 1582, 7170, 1476, 7331]]\n",
            "cleaned data after preprocessing:[[2872 9243 1784 ...    0    0    0]\n",
            " [5285 7047   76 ...    0    0    0]\n",
            " [4067 5285 5180 ...    0    0    0]\n",
            " ...\n",
            " [6101 4510 6101 ...    0    0    0]\n",
            " [6101 7136 2422 ...    0    0    0]\n",
            " [  60 8092 8116 ...    0    0    0]]\n",
            "train_inde_data shape: (7000, 953)\n",
            "self.train_dep_data shape: (7000,)\n",
            "self.val_inde_data shape: (2000, 953)\n",
            "self.val_dep_data shape: (2000,)\n",
            "self.test_inde_data shape: (1000, 953)\n",
            "self.test_dep_data shape: (1000,)\n",
            "Epoch 1/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 408ms/step - accuracy: 0.5555 - loss: 0.6998 - val_accuracy: 0.7235 - val_loss: 0.5721\n",
            "Epoch 2/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.8059 - loss: 0.4746 - val_accuracy: 0.7805 - val_loss: 0.5006\n",
            "Epoch 3/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 374ms/step - accuracy: 0.8842 - loss: 0.3465 - val_accuracy: 0.7805 - val_loss: 0.4976\n",
            "Epoch 4/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9170 - loss: 0.2741 - val_accuracy: 0.7785 - val_loss: 0.5159\n",
            "Epoch 5/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.9415 - loss: 0.2193 - val_accuracy: 0.7720 - val_loss: 0.5568\n",
            "Epoch 6/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9511 - loss: 0.1899 - val_accuracy: 0.7610 - val_loss: 0.5984\n",
            "Epoch 7/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 374ms/step - accuracy: 0.9609 - loss: 0.1595 - val_accuracy: 0.7560 - val_loss: 0.6457\n",
            "Epoch 8/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 374ms/step - accuracy: 0.9627 - loss: 0.1491 - val_accuracy: 0.7670 - val_loss: 0.6388\n",
            "Epoch 9/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 374ms/step - accuracy: 0.9725 - loss: 0.1225 - val_accuracy: 0.7665 - val_loss: 0.6783\n",
            "Epoch 10/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9775 - loss: 0.1010 - val_accuracy: 0.7590 - val_loss: 0.7224\n",
            "Epoch 11/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.9789 - loss: 0.0929 - val_accuracy: 0.7595 - val_loss: 0.7301\n",
            "Epoch 12/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.9817 - loss: 0.0813 - val_accuracy: 0.7575 - val_loss: 0.7503\n",
            "Epoch 13/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9861 - loss: 0.0677 - val_accuracy: 0.7575 - val_loss: 0.7712\n",
            "Epoch 14/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9891 - loss: 0.0583 - val_accuracy: 0.7565 - val_loss: 0.7919\n",
            "Epoch 15/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 388ms/step - accuracy: 0.9848 - loss: 0.0617 - val_accuracy: 0.7615 - val_loss: 0.7887\n",
            "Epoch 16/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.9866 - loss: 0.0546 - val_accuracy: 0.7625 - val_loss: 0.8204\n",
            "Epoch 17/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9922 - loss: 0.0399 - val_accuracy: 0.7560 - val_loss: 0.8538\n",
            "Epoch 18/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 388ms/step - accuracy: 0.9926 - loss: 0.0377 - val_accuracy: 0.7575 - val_loss: 0.8602\n",
            "Epoch 19/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - accuracy: 0.9926 - loss: 0.0377 - val_accuracy: 0.7610 - val_loss: 0.8851\n",
            "Epoch 20/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 373ms/step - accuracy: 0.9930 - loss: 0.0350 - val_accuracy: 0.7540 - val_loss: 0.9341\n"
          ]
        }
      ],
      "source": [
        "class sent_analysis:\n",
        "    def __init__(self):\n",
        "        self.df=pd.read_csv(r'/content/IMDB Dataset.csv')\n",
        "        self.df=self.df.iloc[:10000]\n",
        "        print(f'shape{self.df.shape}')\n",
        "        print(self.df.columns)\n",
        "        print(self.df.isnull().sum())\n",
        "        self.df['sentiment']=self.df['sentiment'].map({'positive':1,'negative':0})\n",
        "        print(self.df.head(5))\n",
        "        print(self.df['sentiment'].value_counts())\n",
        "    def data_cleaning(self):\n",
        "        try:\n",
        "            self.cleaned_data = []\n",
        "            for i in self.df['review']:\n",
        "                data = i.lower()\n",
        "                data = ''.join([i for i in data if i not in string.punctuation])\n",
        "                data = ' '.join([lemma.lemmatize(i) for i in data.split() if i not in stopwords.words('english')])\n",
        "                self.cleaned_data.append(data)\n",
        "            print(f'data after cleaning:{self.cleaned_data[:10]}')\n",
        "\n",
        "        except Exception as e:\n",
        "            er_ty, er_msg, er_lin = sys.exc_info()\n",
        "            print(f\"Performance Error : {er_lin.tb_lineno} : due to {er_msg}\")\n",
        "\n",
        "    def vectors_conversion(self):\n",
        "        try:\n",
        "          data_size=10000\n",
        "          self.vec_data=[one_hot(i,data_size)for i in self.cleaned_data]\n",
        "          print(f'data after one hot{self.vec_data[:10]}')\n",
        "          self.t=[]\n",
        "          for i in self.vec_data:\n",
        "            self.t.append(len(i))\n",
        "          self.a=max(self.t)\n",
        "          self.final_data = pad_sequences(self.vec_data,maxlen=self.a,padding='post')\n",
        "          print(f'cleaned data after preprocessing:{self.final_data}')\n",
        "\n",
        "        except Exception as e:\n",
        "            er_ty, er_msg, er_lin = sys.exc_info()\n",
        "            print(f\"Performance Error : {er_lin.tb_lineno} : due to {er_msg}\")\n",
        "    def data_splitting(self):\n",
        "      try:\n",
        "        self.train_inde_data=self.final_data[:7000]\n",
        "        self.train_dep_data=self.df['sentiment'][:7000]\n",
        "        self.val_inde_data=self.final_data[7000:9000]\n",
        "        self.val_dep_data=self.df['sentiment'][7000:9000]\n",
        "        self.test_inde_data=self.final_data[9000:]\n",
        "        self.test_dep_data=self.df['sentiment'][9000:]\n",
        "        print(f'train_inde_data shape: {self.train_inde_data.shape}')\n",
        "        print(f'self.train_dep_data shape: {self.train_dep_data.shape}')\n",
        "        print(f'self.val_inde_data shape: {self.val_inde_data.shape}')\n",
        "        print(f'self.val_dep_data shape: {self.val_dep_data.shape}')\n",
        "        print(f'self.test_inde_data shape: {self.test_inde_data.shape}')\n",
        "        print(f'self.test_dep_data shape: {self.test_dep_data.shape}')\n",
        "      except Exception as e:\n",
        "          er_ty, er_msg, er_lin = sys.exc_info()\n",
        "          print(f\"Performance Error : {er_lin.tb_lineno} : due to {er_msg}\")\n",
        "\n",
        "    def model_training(self):\n",
        "      try:\n",
        "        self.model=Sequential()\n",
        "        self.model.add(Embedding(input_dim=10000,output_dim=5,input_length=self.a))\n",
        "        self.model.add(Masking(mask_value=0.0))\n",
        "        self.model.add(Bidirectional(SimpleRNN(units = 3,return_sequences=True,name='Hidden_layer_1')))\n",
        "        self.model.add(Bidirectional(SimpleRNN(units = 4,return_sequences=True,name='Hidden_layer_2')))\n",
        "        self.model.add(Bidirectional(SimpleRNN(units = 5,return_sequences=False,name='Hidden_layer_3')))\n",
        "        self.model.add(Dense(units = 1,activation='sigmoid',name='output_layer'))\n",
        "\n",
        "        self.model.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "        self.model.fit(self.train_inde_data,self.train_dep_data,epochs=20,batch_size=100,validation_data=(self.val_inde_data,self.val_dep_data))\n",
        "      except Exception as e:\n",
        "          er_ty, er_msg, er_lin = sys.exc_info()\n",
        "          print(f\"Performance Error : {er_lin.tb_lineno} : due to {er_msg}\")\n",
        "    def model_save(self):\n",
        "      try:\n",
        "        with open('analysis.pkl','wb') as f:\n",
        "          pickle.dump(self.model,f)\n",
        "      except Exception as e:\n",
        "          er_ty, er_msg, er_lin = sys.exc_info()\n",
        "          print(f\"Performance Error : {er_lin.tb_lineno} : due to {er_msg}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    obj=sent_analysis()\n",
        "    obj.data_cleaning()\n",
        "    obj.vectors_conversion()\n",
        "    obj.data_splitting()\n",
        "    obj.model_training()\n",
        "    obj.model_save()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('analysis.pkl','rb') as f:\n",
        "  m = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "3hyFoThdnDaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=['positive','negative']"
      ],
      "metadata": {
        "id": "rwlG1whaTRlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size=10000\n",
        "review=['I was very lucky to see this film as part of the Melbourne International Film Festival 2005 only a few days ago. I must admit that I am very partial to movies that focus on human relations and especially the ones which concentrate on the tragic side of life. I also love the majority of Scandinavian cinematic offerings, there is often a particular deep quality in the way the story unfolds and the characters are drawn. Character building in this film is extraordinary in its details and its depth. This is despite the fact that we do encounter quite a number of characters all with very particular personal situations and locations within their community. The audience at the end of the screening was very silent and pensive. I am still playing some of those scenes in my mind and I am still amazed at their power and meaningfulness.']\n",
        "text = review[0].lower()\n",
        "text = ''.join([i for i in text if i not in string.punctuation])\n",
        "text = ' '.join([lemma.lemmatize(i) for i in text.split() if i not in stopwords.words('english')])\n",
        "v = [one_hot(i,dic_size) for i in [text]]\n",
        "p = pad_sequences(v,maxlen=953,padding='post')\n",
        "print(labels[np.argmax(m.predict(p))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9Drn6NSLOu",
        "outputId": "93e1541c-7c6b-4721-8c01-3c31e33b0685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "positive\n"
          ]
        }
      ]
    }
  ]
}